{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3082197b",
   "metadata": {},
   "source": [
    "# Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac7399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from factor_analyzer import FactorAnalyzer, calculate_kmo\n",
    "from scipy.stats import bartlett\n",
    "\n",
    "# Load the data from your CSV file\n",
    "file_name = 'data.csv'\n",
    "data = pd.read_csv(file_name)\n",
    "\n",
    "# Clean and preprocess the data (adjust the columns to your data)\n",
    "data.columns = data.columns.str.strip().str.upper()\n",
    "\n",
    "# Select the relevant question columns for analysis (customize this part based on your variables)\n",
    "questions_columns = [col for col in data.columns if col.startswith(('FAS', 'LOC', 'S', 'QOA', 'I', 'M', 'STS', 'IPS', 'SSRL', 'SDS'))]\n",
    "questions_data = data[questions_columns]\n",
    "\n",
    "# Handle missing values by filling with column means\n",
    "questions_data.fillna(questions_data.mean(), inplace=True)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "questions_data_scaled = scaler.fit_transform(questions_data)\n",
    "\n",
    "# Perform KMO and Bartlett's tests\n",
    "kmo_all, kmo_model = calculate_kmo(questions_data_scaled)\n",
    "print(f\"KMO Test Score: {kmo_model:.3f}\")\n",
    "\n",
    "chi_square_value, p_value = bartlett(*[questions_data[col] for col in questions_columns])\n",
    "print(f\"Bartlettâ€™s Test p-value: {p_value:.3f}\")\n",
    "\n",
    "# Factor Analysis\n",
    "fa = FactorAnalyzer(rotation=None)\n",
    "fa.fit(questions_data_scaled)\n",
    "\n",
    "# Get eigenvalues to determine the optimal number of factors\n",
    "eigenvalues, _ = fa.get_eigenvalues()\n",
    "\n",
    "# Plot Scree Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(eigenvalues) + 1), eigenvalues, marker='o', linestyle='--')\n",
    "plt.axhline(y=1, color='r', linestyle='-')\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Factors')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.show()\n",
    "\n",
    "# Determine the optimal number of factors based on eigenvalues > 1\n",
    "optimal_factors = sum(eigenvalues > 1)\n",
    "print(f'Optimal number of factors: {optimal_factors}')\n",
    "\n",
    "# Perform Factor Analysis with the optimal number of factors\n",
    "fa_optimal = FactorAnalyzer(n_factors=optimal_factors, rotation='promax')\n",
    "fa_optimal.fit(questions_data_scaled)\n",
    "\n",
    "# Get Factor Loadings\n",
    "factor_loadings = pd.DataFrame(fa_optimal.loadings_, index=questions_columns)\n",
    "\n",
    "# Save factor loadings to CSV\n",
    "factor_loadings.to_csv('factor_loadings.csv')\n",
    "\n",
    "# Assign sequential group numbers\n",
    "factor_loadings_df = factor_loadings.copy()\n",
    "factor_loadings_df['Group'] = np.argmax(fa_optimal.loadings_, axis=1) + 1\n",
    "\n",
    "# Ensure no missing factor numbers\n",
    "unique_factors = np.unique(factor_loadings_df['Group'])\n",
    "factor_mapping = {old_group: new_group for new_group, old_group in enumerate(sorted(unique_factors), start=1)}\n",
    "factor_loadings_df['Group'] = factor_loadings_df['Group'].map(factor_mapping)\n",
    "\n",
    "# Create a DataFrame for grouped questions\n",
    "grouped_questions_df = factor_loadings_df.reset_index()\n",
    "grouped_questions_df.columns = ['Question'] + [f'Factor_{i + 1}' for i in range(optimal_factors)] + ['Group']\n",
    "grouped_questions_df = grouped_questions_df[['Question', 'Group']]\n",
    "\n",
    "# Sort the DataFrame by 'Group' column\n",
    "grouped_questions_df.sort_values(by='Group', inplace=True)\n",
    "\n",
    "# Reset index after sorting\n",
    "grouped_questions_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Rename the Group column for clarity\n",
    "grouped_questions_df.rename(columns={'Group': 'Factor Group'}, inplace=True)\n",
    "\n",
    "# Plot the factor loadings\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a color palette for the factors\n",
    "palette = sns.color_palette(\"husl\", len(grouped_questions_df['Factor Group'].unique()))\n",
    "\n",
    "# Plot each factor with a different color\n",
    "for factor, color in zip(grouped_questions_df['Factor Group'].unique(), palette):\n",
    "    subset = grouped_questions_df[grouped_questions_df['Factor Group'] == factor]\n",
    "    plt.scatter(subset['Question'], [factor] * len(subset), label=f'Factor {factor}', s=100, edgecolor='k', color=color)\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Question')\n",
    "plt.ylabel('Factor Group')\n",
    "plt.title('Grouping of Questions by Factors')\n",
    "plt.legend(title='Factor Group')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('grouped_questions_plot.png')\n",
    "plt.show()\n",
    "\n",
    "# Save the grouped questions to CSV\n",
    "grouped_questions_df.to_csv('grouped_questions.csv', index=False)\n",
    "\n",
    "# Display the grouped questions\n",
    "print(\"Grouped Questions by Factor:\")\n",
    "print(grouped_questions_df)\n",
    "\n",
    "# Load factor loadings data from CSV\n",
    "factor_loadings_df = pd.read_csv('factor_loadings.csv', index_col=0)\n",
    "\n",
    "# Create a DataFrame for mapping groups\n",
    "group_mapping = dict(zip(grouped_questions_df['Question'], grouped_questions_df['Factor Group']))\n",
    "\n",
    "# Convert factor loadings to a NumPy array for faster calculations\n",
    "factor_loadings_array = factor_loadings_df.iloc[:, :-1].values\n",
    "\n",
    "# Calculate mean distances between each pair of groups\n",
    "unique_groups = grouped_questions_df['Factor Group'].unique()\n",
    "\n",
    "distance_matrix = pd.DataFrame(np.zeros((len(unique_groups), len(unique_groups))), \n",
    "                               index=unique_groups, columns=unique_groups)\n",
    "\n",
    "group_ids = [group_mapping[q] for q in factor_loadings_df.index]\n",
    "\n",
    "for group_i in unique_groups:\n",
    "    for group_j in unique_groups:\n",
    "        if group_i != group_j:\n",
    "            indices_i = [idx for idx, group in enumerate(group_ids) if group == group_i]\n",
    "            indices_j = [idx for idx, group in enumerate(group_ids) if group == group_j]\n",
    "\n",
    "            distances = []\n",
    "            \n",
    "            for idx_i in indices_i:\n",
    "                for idx_j in indices_j:\n",
    "                    dist = np.abs(factor_loadings_array[idx_i] - factor_loadings_array[idx_j])\n",
    "                    distances.extend(dist)\n",
    "            \n",
    "            mean_distance = np.mean(distances)\n",
    "            distance_matrix.loc[group_i, group_j] = mean_distance\n",
    "\n",
    "# Display the matrix showing the mean distances between each pair of groups\n",
    "print(\"Mean Distances Between Groups:\")\n",
    "print(distance_matrix)\n",
    "\n",
    "# Save the distance matrix to CSV\n",
    "distance_matrix.to_csv('mean_distances_between_groups.csv')\n",
    "\n",
    "# Calculate mean distances within each group\n",
    "group_mean_distances = {}\n",
    "\n",
    "for group in unique_groups:\n",
    "    group_questions = [q for q in factor_loadings_df.index if group_mapping.get(q) == group]\n",
    "\n",
    "    if len(group_questions) < 2:\n",
    "        group_mean_distances[group] = 0\n",
    "        continue\n",
    "\n",
    "    group_data = factor_loadings_df.loc[group_questions].iloc[:, :-1]\n",
    "\n",
    "    pairwise_diffs = []\n",
    "    for i in range(len(group_questions)):\n",
    "        for j in range(i + 1, len(group_questions)):\n",
    "            dist = np.abs(group_data.iloc[i] - group_data.iloc[j])\n",
    "            pairwise_diffs.extend(dist)\n",
    "\n",
    "    group_mean_distance = np.mean(pairwise_diffs)\n",
    "    group_mean_distances[group] = group_mean_distance\n",
    "\n",
    "# Display mean distances within each group\n",
    "for group, mean_distance in group_mean_distances.items():\n",
    "    print(f\"Mean distance within group {group}: {mean_distance:.4f}\")\n",
    "\n",
    "# Save the mean distances to a CSV file\n",
    "mean_distances_df = pd.DataFrame.from_dict(group_mean_distances, orient='index', columns=['Mean Distance'])\n",
    "mean_distances_df.to_csv('mean_distances_within_groups.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c511d3",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebddb06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load your data from the CSV file\n",
    "data = pd.read_csv('data.csv')  # Update 'data.csv' with the actual filename\n",
    "\n",
    "# Check for missing values\n",
    "if data.isnull().sum().any():\n",
    "    print(\"Missing values found. Handling missing values...\")\n",
    "    data = data.dropna()  # Optionally, you can fill missing values instead\n",
    "\n",
    "# Complete variable mapping including all relevant columns for each construct\n",
    "var_mapping = {\n",
    "    'Autonomy': ['SDS1', 'SDS4', 'SDS8', 'SDS11', 'SDS14', 'SDS17', 'SDS20'],\n",
    "    'Competence': ['SDS3', 'SDS5', 'SDS10', 'SDS13', 'SDS15', 'SDS19'],\n",
    "    'Relatedness': ['SDS6', 'SDS7', 'SDS9', 'SDS12', 'SDS16', 'SDS18', 'SDS21'],\n",
    "    'Self_Regulation_Learning': ['SSRL1', 'SSRL2', 'SSRL3', 'SSRL4', 'SSRL5', \n",
    "                                  'SSRL6', 'SSRL7', 'SSRL8', 'SSRL9', 'SSRL10', \n",
    "                                  'SSRL11', 'SSRL12', 'SSRL13', 'SSRL14', \n",
    "                                  'SSRL15', 'SSRL16', 'SSRL17'],\n",
    "    'Irrational_Procrastination': ['IPS1', 'IPS2', 'IPS3', 'IPS4', 'IPS5', \n",
    "                                    'IPS6', 'IPS7', 'IPS8', 'IPS9'],\n",
    "    'Susceptibility_to_Temptation': ['STS1', 'STS2', 'STS3', 'STS4', 'STS5', \n",
    "                                      'STS6', 'STS7', 'STS8', 'STS9', 'STS10', 'STS11'],\n",
    "    'Academic_Commitment': ['LOC1', 'LOC2', 'LOC3', 'LOC4', 'LOC5', 'S1', 'S2', \n",
    "                            'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'QOA1', \n",
    "                            'QOA2', 'QOA3', 'I1', 'I2', 'I3', 'I4', \n",
    "                            'I5', 'M1', 'M2', 'M3', 'M4', 'M5', \n",
    "                            'M6', 'M7', 'M8', 'M9'],\n",
    "    'Family_Academic_Support': ['FAS1', 'FAS2', 'FAS3', 'FAS4', 'FAS5', \n",
    "                                 'FAS6', 'FAS7', 'FAS8', 'FAS9', 'FAS10', \n",
    "                                 'FAS11', 'FAS12', 'FAS13', 'FAS14', 'FAS15']\n",
    "}\n",
    "\n",
    "# Define the hypotheses with their corresponding columns\n",
    "hypotheses = [\n",
    "    {\"IV\": var_mapping['Competence'] + var_mapping['Academic_Commitment'], \"DV\": var_mapping['Irrational_Procrastination'], \"hypothesis_num\": \"H1\"},\n",
    "    {\"IV\": var_mapping['Competence'] + var_mapping['Self_Regulation_Learning'], \"DV\": var_mapping['Irrational_Procrastination'], \"hypothesis_num\": \"H2\"},\n",
    "    {\"IV\": var_mapping['Competence'] + var_mapping['Family_Academic_Support'], \"DV\": var_mapping['Irrational_Procrastination'], \"hypothesis_num\": \"H3\"},\n",
    "    \n",
    "    {\"IV\": var_mapping['Competence'] + var_mapping['Academic_Commitment'], \"DV\": var_mapping['Susceptibility_to_Temptation'], \"hypothesis_num\": \"H4\"},\n",
    "    {\"IV\": var_mapping['Competence'] + var_mapping['Self_Regulation_Learning'], \"DV\": var_mapping['Susceptibility_to_Temptation'], \"hypothesis_num\": \"H5\"},\n",
    "    {\"IV\": var_mapping['Competence'] + var_mapping['Family_Academic_Support'], \"DV\": var_mapping['Susceptibility_to_Temptation'], \"hypothesis_num\": \"H6\"},\n",
    "    \n",
    "    {\"IV\": var_mapping['Autonomy'] + var_mapping['Academic_Commitment'], \"DV\": var_mapping['Irrational_Procrastination'], \"hypothesis_num\": \"H7\"},\n",
    "    {\"IV\": var_mapping['Autonomy'] + var_mapping['Self_Regulation_Learning'], \"DV\": var_mapping['Irrational_Procrastination'], \"hypothesis_num\": \"H8\"},\n",
    "    {\"IV\": var_mapping['Autonomy'] + var_mapping['Family_Academic_Support'], \"DV\": var_mapping['Irrational_Procrastination'], \"hypothesis_num\": \"H9\"},\n",
    "    \n",
    "    {\"IV\": var_mapping['Autonomy'] + var_mapping['Academic_Commitment'], \"DV\": var_mapping['Susceptibility_to_Temptation'], \"hypothesis_num\": \"H10\"},\n",
    "    {\"IV\": var_mapping['Autonomy'] + var_mapping['Self_Regulation_Learning'], \"DV\": var_mapping['Susceptibility_to_Temptation'], \"hypothesis_num\": \"H11\"},\n",
    "    {\"IV\": var_mapping['Autonomy'] + var_mapping['Family_Academic_Support'], \"DV\": var_mapping['Susceptibility_to_Temptation'], \"hypothesis_num\": \"H12\"},\n",
    "    \n",
    "    {\"IV\": var_mapping['Relatedness'] + var_mapping['Academic_Commitment'], \"DV\": var_mapping['Irrational_Procrastination'], \"hypothesis_num\": \"H13\"},\n",
    "    {\"IV\": var_mapping['Relatedness'] + var_mapping['Self_Regulation_Learning'], \"DV\": var_mapping['Irrational_Procrastination'], \"hypothesis_num\": \"H14\"},\n",
    "    {\"IV\": var_mapping['Relatedness'] + var_mapping['Family_Academic_Support'], \"DV\": var_mapping['Irrational_Procrastination'], \"hypothesis_num\": \"H15\"},\n",
    "    \n",
    "    {\"IV\": var_mapping['Relatedness'] + var_mapping['Academic_Commitment'], \"DV\": var_mapping['Susceptibility_to_Temptation'], \"hypothesis_num\": \"H16\"},\n",
    "    {\"IV\": var_mapping['Relatedness'] + var_mapping['Self_Regulation_Learning'], \"DV\": var_mapping['Susceptibility_to_Temptation'], \"hypothesis_num\": \"H17\"},\n",
    "    {\"IV\": var_mapping['Relatedness'] + var_mapping['Family_Academic_Support'], \"DV\": var_mapping['Susceptibility_to_Temptation'], \"hypothesis_num\": \"H18\"},\n",
    "]\n",
    "\n",
    "# Function to run Gradient Boosting regression\n",
    "def run_gradient_boosting(independent_vars, dependent_var, data):\n",
    "    X = data[independent_vars]  # Selecting the independent variables\n",
    "    Y = data[dependent_var].values.ravel()  # Ensure Y is a 1D array\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Initialize the Gradient Boosting Regressor\n",
    "    model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    Y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    mse = mean_squared_error(Y_test, Y_pred)\n",
    "    r2 = r2_score(Y_test, Y_pred)\n",
    "    \n",
    "    # Get feature importance\n",
    "    feature_importances = model.feature_importances_\n",
    "    \n",
    "    return mse, r2, feature_importances\n",
    "\n",
    "# Set up for visualization\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Run the Gradient Boosting regression for each hypothesis and print the results\n",
    "for i, hypothesis in enumerate(hypotheses):\n",
    "    IVs = hypothesis['IV']\n",
    "    DV = hypothesis['DV'][0]  # Make sure to select the first element as DV\n",
    "    \n",
    "    # Execute the Gradient Boosting regression\n",
    "    mse, r2, feature_importances = run_gradient_boosting(IVs, DV, data)\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Results for Hypothesis {hypothesis['hypothesis_num']}:\")\n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    print(f\"R-squared: {r2:.4f}\")\n",
    "    print(\"Feature Importances:\")\n",
    "    \n",
    "    # Create a DataFrame for feature importances\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': IVs,\n",
    "        'Importance': feature_importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    # Print feature importances\n",
    "    for feature, importance in zip(importance_df['Feature'], importance_df['Importance']):\n",
    "        print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "    # Interpret the results\n",
    "    if r2 < 0:\n",
    "        interpretation = \"The model does not explain any variance in the dependent variable.\"\n",
    "    elif r2 < 0.1:\n",
    "        interpretation = \"The model explains very little variance.\"\n",
    "    elif r2 < 0.3:\n",
    "        interpretation = \"The model explains a modest amount of variance.\"\n",
    "    elif r2 < 0.5:\n",
    "        interpretation = \"The model explains a substantial amount of variance.\"\n",
    "    else:\n",
    "        interpretation = \"The model explains a large amount of variance.\"\n",
    "    \n",
    "    print(f\"Interpretation of R-squared: {interpretation}\\n\")\n",
    "    \n",
    "    # Plot feature importances\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
    "    plt.title(f'Feature Importance for Hypothesis {hypothesis[\"hypothesis_num\"]}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb30884",
   "metadata": {},
   "source": [
    "# Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf1b1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load your data from the CSV file\n",
    "data = pd.read_csv('data.csv')  # Update 'data.csv' with the actual filename\n",
    "\n",
    "# Check for missing values\n",
    "if data.isnull().sum().any():\n",
    "    print(\"Missing values found. Handling missing values...\")\n",
    "    data = data.dropna()  # Optionally, you can fill missing values instead\n",
    "\n",
    "# Complete variable mapping including all relevant columns for each construct\n",
    "var_mapping = {\n",
    "    'Autonomy': ['SDS1', 'SDS4', 'SDS8', 'SDS11', 'SDS14', 'SDS17', 'SDS20'],\n",
    "    'Competence': ['SDS3', 'SDS5', 'SDS10', 'SDS13', 'SDS15', 'SDS19'],\n",
    "    'Relatedness': ['SDS6', 'SDS7', 'SDS9', 'SDS12', 'SDS16', 'SDS18', 'SDS21'],\n",
    "    'Self_Regulation_Learning': ['SSRL1', 'SSRL2', 'SSRL3', 'SSRL4', 'SSRL5', \n",
    "                                  'SSRL6', 'SSRL7', 'SSRL8', 'SSRL9', 'SSRL10', \n",
    "                                  'SSRL11', 'SSRL12', 'SSRL13', 'SSRL14', \n",
    "                                  'SSRL15', 'SSRL16', 'SSRL17'],\n",
    "    'Irrational_Procrastination': ['IPS1', 'IPS2', 'IPS3', 'IPS4', 'IPS5', \n",
    "                                    'IPS6', 'IPS7', 'IPS8', 'IPS9'],\n",
    "    'Susceptibility_to_Temptation': ['STS1', 'STS2', 'STS3', 'STS4', 'STS5', \n",
    "                                      'STS6', 'STS7', 'STS8', 'STS9', 'STS10', 'STS11'],\n",
    "    'Academic_Commitment': ['LOC1', 'LOC2', 'LOC3', 'LOC4', 'LOC5', 'S1', 'S2', \n",
    "                            'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'QOA1', \n",
    "                            'QOA2', 'QOA3', 'I1', 'I2', 'I3', 'I4', \n",
    "                            'I5', 'M1', 'M2', 'M3', 'M4', 'M5', \n",
    "                            'M6', 'M7', 'M8', 'M9'],\n",
    "    'Family_Academic_Support': ['FAS1', 'FAS2', 'FAS3', 'FAS4', 'FAS5', \n",
    "                                 'FAS6', 'FAS7', 'FAS8', 'FAS9', 'FAS10', \n",
    "                                 'FAS11', 'FAS12', 'FAS13', 'FAS14', 'FAS15']\n",
    "}\n",
    "\n",
    "# Define the hypotheses with their corresponding columns\n",
    "hypotheses = [\n",
    "    {\"IV\": var_mapping['Competence'] + var_mapping['Academic_Commitment'], \"DV\": var_mapping['Irrational_Procrastination'], \"hypothesis_num\": \"H1\"},\n",
    "    {\"IV\": var_mapping['Competence'] + var_mapping['Self_Regulation_Learning'], \"DV\": var_mapping['Irrational_Procrastination'], \"hypothesis_num\": \"H2\"},\n",
    "    {\"IV\": var_mapping['Competence'] + var_mapping['Family_Academic_Support'], \"DV\": var_mapping['Irrational_Procrastination'], \"hypothesis_num\": \"H3\"},\n",
    "    \n",
    "    {\"IV\": var_mapping['Competence'] + var_mapping['Academic_Commitment'], \"DV\": var_mapping['Susceptibility_to_Temptation'], \"hypothesis_num\": \"H4\"},\n",
    "    {\"IV\": var_mapping['Competence'] + var_mapping['Self_Regulation_Learning'], \"DV\": var_mapping['Susceptibility_to_Temptation'], \"hypothesis_num\": \"H5\"},\n",
    "    {\"IV\": var_mapping['Competence'] + var_mapping['Family_Academic_Support'], \"DV\": var_mapping['Susceptibility_to_Temptation'], \"hypothesis_num\": \"H6\"},\n",
    "    \n",
    "    {\"IV\": var_mapping['Autonomy'] + var_mapping['Academic_Commitment'], \"DV\": var_mapping['Irrational_Procrastination'], \"hypothesis_num\": \"H7\"},\n",
    "    {\"IV\": var_mapping['Autonomy'] + var_mapping['Self_Regulation_Learning'], \"DV\": var_mapping['Irrational_Procrastination'], \"hypothesis_num\": \"H8\"},\n",
    "    {\"IV\": var_mapping['Autonomy'] + var_mapping['Family_Academic_Support'], \"DV\": var_mapping['Irrational_Procrastination'], \"hypothesis_num\": \"H9\"},\n",
    "    \n",
    "    {\"IV\": var_mapping['Autonomy'] + var_mapping['Academic_Commitment'], \"DV\": var_mapping['Susceptibility_to_Temptation'], \"hypothesis_num\": \"H10\"},\n",
    "    {\"IV\": var_mapping['Autonomy'] + var_mapping['Self_Regulation_Learning'], \"DV\": var_mapping['Susceptibility_to_Temptation'], \"hypothesis_num\": \"H11\"},\n",
    "    {\"IV\": var_mapping['Autonomy'] + var_mapping['Family_Academic_Support'], \"DV\": var_mapping['Susceptibility_to_Temptation'], \"hypothesis_num\": \"H12\"},\n",
    "\n",
    "    {\"IV\": var_mapping['Relatedness'] + var_mapping['Academic_Commitment'], \"DV\": var_mapping['Irrational_Procrastination'], \"hypothesis_num\": \"H13\"},\n",
    "    {\"IV\": var_mapping['Relatedness'] + var_mapping['Self_Regulation_Learning'], \"DV\": var_mapping['Irrational_Procrastination'], \"hypothesis_num\": \"H14\"},\n",
    "    {\"IV\": var_mapping['Relatedness'] + var_mapping['Family_Academic_Support'], \"DV\": var_mapping['Irrational_Procrastination'], \"hypothesis_num\": \"H15\"},\n",
    "    \n",
    "    {\"IV\": var_mapping['Relatedness'] + var_mapping['Academic_Commitment'], \"DV\": var_mapping['Susceptibility_to_Temptation'], \"hypothesis_num\": \"H16\"},\n",
    "    {\"IV\": var_mapping['Relatedness'] + var_mapping['Self_Regulation_Learning'], \"DV\": var_mapping['Susceptibility_to_Temptation'], \"hypothesis_num\": \"H17\"},\n",
    "    {\"IV\": var_mapping['Relatedness'] + var_mapping['Family_Academic_Support'], \"DV\": var_mapping['Susceptibility_to_Temptation'], \"hypothesis_num\": \"H18\"},\n",
    "]\n",
    "\n",
    "# Function to run Gradient Boosting regression\n",
    "def run_gradient_boosting(independent_vars, dependent_var, data):\n",
    "    X = data[independent_vars]  # Selecting the independent variables\n",
    "    Y = data[dependent_var].values.ravel()  # Ensure Y is a 1D array\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Initialize the Gradient Boosting Regressor\n",
    "    model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    Y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    mse = mean_squared_error(Y_test, Y_pred)\n",
    "    r2 = r2_score(Y_test, Y_pred)\n",
    "    \n",
    "    # Get feature importance\n",
    "    feature_importances = model.feature_importances_\n",
    "    \n",
    "    return mse, r2, feature_importances\n",
    "\n",
    "# Set up for visualization\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Run the Gradient Boosting regression for each hypothesis and print the results\n",
    "for i, hypothesis in enumerate(hypotheses):\n",
    "    IVs = hypothesis['IV']\n",
    "    DV = hypothesis['DV'][0]  # Make sure to select the first element as DV\n",
    "    \n",
    "    # Execute the Gradient Boosting regression\n",
    "    mse, r2, feature_importances = run_gradient_boosting(IVs, DV, data)\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Results for Hypothesis {hypothesis['hypothesis_num']}:\")\n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    print(f\"R-squared: {r2:.4f}\")\n",
    "    print(\"Feature Importances:\")\n",
    "    \n",
    "    # Create a DataFrame for feature importances\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': IVs,\n",
    "        'Importance': feature_importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    # Print feature importances\n",
    "    for feature, importance in zip(importance_df['Feature'], importance_df['Importance']):\n",
    "        print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "    # Interpret the results\n",
    "    if r2 < 0:\n",
    "        interpretation = \"The model does not explain any variance in the dependent variable.\"\n",
    "    elif r2 < 0.1:\n",
    "        interpretation = \"The model explains a very small amount of variance in the dependent variable.\"\n",
    "    elif r2 < 0.3:\n",
    "        interpretation = \"The model explains a small amount of variance in the dependent variable.\"\n",
    "    elif r2 < 0.5:\n",
    "        interpretation = \"The model explains a moderate amount of variance in the dependent variable.\"\n",
    "    elif r2 < 0.7:\n",
    "        interpretation = \"The model explains a substantial amount of variance in the dependent variable.\"\n",
    "    else:\n",
    "        interpretation = \"The model explains a high amount of variance in the dependent variable.\"\n",
    "    \n",
    "    print(f\"Interpretation: {interpretation}\\n\")\n",
    "\n",
    "    # Visualization of feature importance\n",
    "    sns.barplot(data=importance_df, x='Importance', y='Feature', palette='viridis')\n",
    "    plt.title(f'Feature Importance for Hypothesis {hypothesis[\"hypothesis_num\"]}')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.xlim(0, importance_df['Importance'].max() * 1.1)  # Set xlim to better visualize importance\n",
    "    plt.grid(axis='x')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
